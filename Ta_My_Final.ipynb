{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wOgJtQrLHBTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969873d7-fd37-424a-9890-07f2fe16e12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b6YBPnZRJ3oy"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import keras\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random \n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import utils as np_utils\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from PIL import Image, ImageOps\n",
        "from numpy import expand_dims\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jk4L7nZSgTDR"
      },
      "outputs": [],
      "source": [
        "bird_class = pd.read_excel('/content/gdrive/MyDrive/DSCI552_Final/Data/Classes.xlsx')\n",
        "y = OneHotEncoder().fit_transform(bird_class).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N1IlQcvWKFff"
      },
      "outputs": [],
      "source": [
        "subdir = []\n",
        "path = os.getcwd()\n",
        "path = os.path.join(path,'/content/gdrive/MyDrive/DSCI552_Final/Data/images')\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "  fullpath = os.path.join(path, folder)\n",
        "  if folder.endswith('.DS_Store'): \n",
        "      pass\n",
        "  else:\n",
        "    subdir.append(fullpath)   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Exploration and Pre-processing"
      ],
      "metadata": {
        "id": "IKsRo5QmD-rc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "13T2jg0-9fVC"
      },
      "outputs": [],
      "source": [
        "#Split data: 70% train, 15% test, 15% validate\n",
        "train_path = []\n",
        "val_path = []\n",
        "test_path = []\n",
        "desired_size = 224\n",
        "\n",
        "for each_folder in subdir:\n",
        "  data_path_list = []\n",
        "  for file in os.listdir(each_folder):\n",
        "    path = os.path.join(each_folder, file)\n",
        "    newPath = path.replace(os.sep, '/')\n",
        "    data_path_list.append(newPath)\n",
        "\n",
        "    image = cv2.imread(newPath)\n",
        "    # image = cv2.normalize(image, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    image = cv2.resize(image, (224,224)) \n",
        "    cv2.imwrite(newPath,image)\n",
        "    \n",
        "  data = pd.DataFrame(data_path_list)\n",
        "  train, validate, test = np.split(data.sample(frac=1, random_state=25), [int(.7*len(data)), int(.85*len(data))])\n",
        "  train_path.append(train[0].values.tolist())\n",
        "  val_path.append(validate[0].values.tolist())\n",
        "  test_path.append(validate[0].values.tolist())\n",
        "\n",
        "train_path = list(itertools.chain(*train_path))\n",
        "val_path = list(itertools.chain(*val_path))\n",
        "test_path = list(itertools.chain(*test_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jHoVuhLZ_4sK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7347ed04-4941-47fb-e216-98af3db1e52f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_path[0]\n",
        "image = cv2.imread(train_path[0])\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CsPwWehedFG_"
      },
      "outputs": [],
      "source": [
        "bird_class = pd.read_excel('/content/gdrive/MyDrive/DSCI552_Final/Data/Classes.xlsx')\n",
        "bird_class = bird_class.rename(columns = {\"Folder Name \": \"Folder\"})\n",
        "bird_class = bird_class.drop(columns = \"Class\")\n",
        "y = OneHotEncoder().fit_transform(bird_class).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zISzfpcw98ma"
      },
      "outputs": [],
      "source": [
        "final_train_list = []\n",
        "final_test_list = []\n",
        "final_val_list = []\n",
        "def resize_onehot(path):\n",
        "  final_list = []\n",
        "  for each_image_path in path:\n",
        "    for index, row in bird_class.iterrows():\n",
        "      if row[\"Folder\"] in each_image_path:\n",
        "        im = cv2.imread(each_image_path)\n",
        "        final_list.append([im, y[index]])\n",
        "  return final_list\n",
        "\n",
        "final_train_list = resize_onehot(train_path)\n",
        "final_test_list = resize_onehot(test_path)\n",
        "final_val_list = resize_onehot(val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-clcZPamkGJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368122f9-9aa8-4ccc-a607-b3fec2649f26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "820"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(final_train_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hQXvCy7lfzLg"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_val = []\n",
        "y_val = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for each in final_train_list:\n",
        "  x_train.append(each[0])\n",
        "  y_train.append(each[1])\n",
        "for each in final_test_list:\n",
        "  x_test.append(each[0])\n",
        "  y_test.append(each[1])\n",
        "for each in final_val_list:\n",
        "  x_val.append(each[0])\n",
        "  y_val.append(each[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yBtEGt4ZscO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f46aad-f2b3-4b61-9550-32ca5faae6a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "820"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R3DCUOH0MdAh"
      },
      "outputs": [],
      "source": [
        "#Rotation\n",
        "def rotation(each_path):\n",
        "  image = cv2.imread(each_path)\n",
        "  height, width = image.shape[:2]\n",
        "  center = (width/2, height/2)\n",
        "\n",
        "  rotate_matrix = cv2.getRotationMatrix2D(center=center, angle=45, scale=1)\n",
        "  rotated_image = cv2.warpAffine(src=image, M=rotate_matrix, dsize=(width, height))\n",
        "\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(rotated_image)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([rotated_image, y[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gJ-zkGRIMs7c"
      },
      "outputs": [],
      "source": [
        "#Translate\n",
        "def translate(each_path):\n",
        "  image = cv2.imread(each_path)\n",
        "  height, width = image.shape[:2]\n",
        "  tx, ty = width / 4, height / 4\n",
        "  translation_matrix = np.array([\n",
        "      [1, 0, tx],\n",
        "      [0, 1, ty]\n",
        "  ], dtype=np.float32)\n",
        "  translated_image = cv2.warpAffine(src=image, M=translation_matrix, dsize=(width, height))\n",
        "\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(translated_image)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([translated_image, y[index]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8r3dUdEZO6tW"
      },
      "outputs": [],
      "source": [
        "#Crop\n",
        "desired_size = 224\n",
        "def crop(each_path):\n",
        "  img = cv2.imread(each_path)\n",
        "  cropped_image = img[50:174, 50:174]\n",
        "  cropped_image = cv2.resize(cropped_image, (224,224)) \n",
        "\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(cropped_image)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([cropped_image, y[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rnR2kuSJRl0E"
      },
      "outputs": [],
      "source": [
        "#Randomly zoom\n",
        "def zoom(each_path):\n",
        "  image = load_img(each_path)\n",
        "  dataImage = img_to_array(image)\n",
        "  imageNew = expand_dims(dataImage, 0)\n",
        "\n",
        "  imageDataGen = ImageDataGenerator(zoom_range=[0.7,1.0])\n",
        "  iterator = imageDataGen.flow(imageNew, batch_size=1)\n",
        "  batch = iterator.next()\n",
        "  image = batch[0].astype('uint8')\n",
        "\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(image)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([image, y[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XzTvu4-8V-un"
      },
      "outputs": [],
      "source": [
        "#flip\n",
        "def flip(each_path):\n",
        "  img = cv2.imread(each_path)\n",
        "  vertical_img = cv2.flip(img, 0)\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(vertical_img)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([vertical_img, y[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yztrztJ-Wzud"
      },
      "outputs": [],
      "source": [
        "#Contrast\n",
        "def contrast(each_path):\n",
        "  img = cv2.imread(each_path)\n",
        "  lab = cv2.cvtColor(img,cv2.COLOR_BGR2LAB)\n",
        "  l_channel, a, b = cv2.split(lab)\n",
        "\n",
        "  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "  cl = clahe.apply(l_channel)\n",
        "\n",
        "  limg = cv2.merge((cl,a,b))\n",
        "  enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR) \n",
        "\n",
        "  for index, row in bird_class.iterrows():\n",
        "    if row[\"Folder\"] in each_path:\n",
        "      x_train.append(enhanced_img)\n",
        "      y_train.append(y[index])\n",
        "      final_train_list.append([enhanced_img, y[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oxkoo4Q_nb8o"
      },
      "outputs": [],
      "source": [
        "for each_path in train_path:\n",
        "  k_num = random.randint(1,6)\n",
        "  aug = (rotation, translate, crop, zoom, flip, contrast)\n",
        "  aug_list = random.choices(aug, k=6)\n",
        "  for each_aug in aug_list:\n",
        "    each_aug(each_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ToHOAWYfU-1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa141987-8eff-4913-bafa-2669471672e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "t4Cz5JnzlosT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8eb1c26-e1a3-481e-ed79-b796d603f315"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5740"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBkOIEdB0XC"
      },
      "source": [
        "Use ReLU activation functions in the last layer and a softmax layer, along\n",
        "with batch normalization\n",
        "4\n",
        "and a dropout rate of 20% as well as ADAM optimizer. Use multinomial cross entropy loss. You can try any batch size,\n",
        "but a batch size of 5 seems reasonable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Lj6ZSKAFq5nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64699ba-d2cc-40c4-d82e-2be370a19486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet',pooling='avg',classes=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "To7xk0glm1LN"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(base_model)\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "# model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Dense(20, activation=('softmax')))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "model.build(input_shape=(224,224,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2bdCiovwsj8",
        "outputId": "787b0595-61b4-4c62-9188-031d4de77f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 512)               14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 512)              2048      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 20)                10260     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,727,416\n",
            "Trainable params: 11,704\n",
            "Non-trainable params: 14,715,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ3JJcHUoxok",
        "outputId": "626c0658-0740-4416-bdc1-1e3af8ba7a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "x_train = tf.stack(x_train)\n",
        "y_train = tf.stack(y_train)\n",
        "x_val = tf.stack(x_val)\n",
        "y_val = tf.stack(y_val)\n",
        "x_train1 = tf.keras.applications.vgg16.preprocess_input(x_train)\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "epochs = 100\n",
        "decay = initial_learning_rate / epochs\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return lr * 1 / (1 + decay * epoch)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K7GfMj53jQYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ac6025-2441-4c05-afc7-fb2e0382194e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 3.2008 - accuracy: 0.0660\n",
            "Epoch 1: val_accuracy improved from -inf to 0.08427, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 43s 142ms/step - loss: 3.2008 - accuracy: 0.0660 - val_loss: 3.1610 - val_accuracy: 0.0843 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.998999847394012e-05.\n",
            "Epoch 2/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.8222 - accuracy: 0.1425\n",
            "Epoch 2: val_accuracy improved from 0.08427 to 0.19101, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 31s 134ms/step - loss: 2.8222 - accuracy: 0.1425 - val_loss: 2.7314 - val_accuracy: 0.1910 - lr: 9.9990e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.997000630676428e-05.\n",
            "Epoch 3/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.5498 - accuracy: 0.2247\n",
            "Epoch 3: val_accuracy improved from 0.19101 to 0.24719, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 2.5498 - accuracy: 0.2247 - val_loss: 2.4381 - val_accuracy: 0.2472 - lr: 9.9970e-05\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.994002396931107e-05.\n",
            "Epoch 4/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.3212 - accuracy: 0.2960\n",
            "Epoch 4: val_accuracy improved from 0.24719 to 0.30899, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 2.3212 - accuracy: 0.2960 - val_loss: 2.2056 - val_accuracy: 0.3090 - lr: 9.9940e-05\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.990006173048161e-05.\n",
            "Epoch 5/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.1142 - accuracy: 0.3791\n",
            "Epoch 5: val_accuracy improved from 0.30899 to 0.37079, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 137ms/step - loss: 2.1142 - accuracy: 0.3791 - val_loss: 2.0193 - val_accuracy: 0.3708 - lr: 9.9900e-05\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 9.985013712739302e-05.\n",
            "Epoch 6/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.9389 - accuracy: 0.4338\n",
            "Epoch 6: val_accuracy improved from 0.37079 to 0.42697, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 1.9389 - accuracy: 0.4338 - val_loss: 1.8645 - val_accuracy: 0.4270 - lr: 9.9850e-05\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.979026041855706e-05.\n",
            "Epoch 7/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.7832 - accuracy: 0.4934\n",
            "Epoch 7: val_accuracy improved from 0.42697 to 0.45506, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.7832 - accuracy: 0.4934 - val_loss: 1.7521 - val_accuracy: 0.4551 - lr: 9.9790e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.972045640012098e-05.\n",
            "Epoch 8/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.6196 - accuracy: 0.5458\n",
            "Epoch 8: val_accuracy improved from 0.45506 to 0.49438, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.6196 - accuracy: 0.5458 - val_loss: 1.6614 - val_accuracy: 0.4944 - lr: 9.9720e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 9.964074258818005e-05.\n",
            "Epoch 9/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.5014 - accuracy: 0.5704\n",
            "Epoch 9: val_accuracy improved from 0.49438 to 0.51124, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 1.5014 - accuracy: 0.5704 - val_loss: 1.5823 - val_accuracy: 0.5112 - lr: 9.9641e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 9.955114376124448e-05.\n",
            "Epoch 10/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.3965 - accuracy: 0.5997\n",
            "Epoch 10: val_accuracy improved from 0.51124 to 0.55056, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.3965 - accuracy: 0.5997 - val_loss: 1.5238 - val_accuracy: 0.5506 - lr: 9.9551e-05\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 9.945169195661194e-05.\n",
            "Epoch 11/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.2966 - accuracy: 0.6287\n",
            "Epoch 11: val_accuracy improved from 0.55056 to 0.57303, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.2966 - accuracy: 0.6287 - val_loss: 1.4666 - val_accuracy: 0.5730 - lr: 9.9452e-05\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 9.934241193081636e-05.\n",
            "Epoch 12/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.2064 - accuracy: 0.6554\n",
            "Epoch 12: val_accuracy improved from 0.57303 to 0.58427, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 1.2064 - accuracy: 0.6554 - val_loss: 1.4315 - val_accuracy: 0.5843 - lr: 9.9342e-05\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 9.922334296497487e-05.\n",
            "Epoch 13/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.1385 - accuracy: 0.6751\n",
            "Epoch 13: val_accuracy improved from 0.58427 to 0.60112, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.1385 - accuracy: 0.6751 - val_loss: 1.4047 - val_accuracy: 0.6011 - lr: 9.9223e-05\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 9.909451705800132e-05.\n",
            "Epoch 14/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.0841 - accuracy: 0.6864\n",
            "Epoch 14: val_accuracy improved from 0.60112 to 0.60674, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 1.0841 - accuracy: 0.6864 - val_loss: 1.3731 - val_accuracy: 0.6067 - lr: 9.9095e-05\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 9.895598072759898e-05.\n",
            "Epoch 15/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.0136 - accuracy: 0.7071\n",
            "Epoch 15: val_accuracy did not improve from 0.60674\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 1.0136 - accuracy: 0.7071 - val_loss: 1.3633 - val_accuracy: 0.6067 - lr: 9.8956e-05\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 9.880776594277179e-05.\n",
            "Epoch 16/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.7220\n",
            "Epoch 16: val_accuracy improved from 0.60674 to 0.61798, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.9611 - accuracy: 0.7220 - val_loss: 1.3484 - val_accuracy: 0.6180 - lr: 9.8808e-05\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 9.864992645276061e-05.\n",
            "Epoch 17/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.7289\n",
            "Epoch 17: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.9157 - accuracy: 0.7289 - val_loss: 1.3401 - val_accuracy: 0.6124 - lr: 9.8650e-05\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 9.84825087217336e-05.\n",
            "Epoch 18/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.8776 - accuracy: 0.7462\n",
            "Epoch 18: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.8776 - accuracy: 0.7462 - val_loss: 1.3321 - val_accuracy: 0.6180 - lr: 9.8483e-05\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 9.830555919530664e-05.\n",
            "Epoch 19/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.8371 - accuracy: 0.7573\n",
            "Epoch 19: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.8371 - accuracy: 0.7573 - val_loss: 1.3373 - val_accuracy: 0.6067 - lr: 9.8306e-05\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 9.81191315627121e-05.\n",
            "Epoch 20/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.7909 - accuracy: 0.7666\n",
            "Epoch 20: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.7909 - accuracy: 0.7666 - val_loss: 1.3387 - val_accuracy: 0.6067 - lr: 9.8119e-05\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 9.792328675318427e-05.\n",
            "Epoch 21/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.7658 - accuracy: 0.7700\n",
            "Epoch 21: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.7658 - accuracy: 0.7700 - val_loss: 1.3394 - val_accuracy: 0.6067 - lr: 9.7923e-05\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 9.771807841092672e-05.\n",
            "Epoch 22/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.7246 - accuracy: 0.7869\n",
            "Epoch 22: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.7246 - accuracy: 0.7869 - val_loss: 1.3215 - val_accuracy: 0.6180 - lr: 9.7718e-05\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 9.750356741871807e-05.\n",
            "Epoch 23/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.7841\n",
            "Epoch 23: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 137ms/step - loss: 0.7115 - accuracy: 0.7841 - val_loss: 1.3262 - val_accuracy: 0.6180 - lr: 9.7504e-05\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 9.727982189430099e-05.\n",
            "Epoch 24/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6799 - accuracy: 0.7983\n",
            "Epoch 24: val_accuracy did not improve from 0.61798\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.6799 - accuracy: 0.7983 - val_loss: 1.3289 - val_accuracy: 0.6180 - lr: 9.7280e-05\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 9.704690992823636e-05.\n",
            "Epoch 25/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.8000\n",
            "Epoch 25: val_accuracy improved from 0.61798 to 0.62921, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.6707 - accuracy: 0.8000 - val_loss: 1.3163 - val_accuracy: 0.6292 - lr: 9.7047e-05\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 9.680489958391672e-05.\n",
            "Epoch 26/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6372 - accuracy: 0.8118\n",
            "Epoch 26: val_accuracy improved from 0.62921 to 0.63483, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 139ms/step - loss: 0.6372 - accuracy: 0.8118 - val_loss: 1.3216 - val_accuracy: 0.6348 - lr: 9.6805e-05\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 9.655385889757983e-05.\n",
            "Epoch 27/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.8251\n",
            "Epoch 27: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.6085 - accuracy: 0.8251 - val_loss: 1.3311 - val_accuracy: 0.6236 - lr: 9.6554e-05\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 9.629386313468771e-05.\n",
            "Epoch 28/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.8159\n",
            "Epoch 28: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.6106 - accuracy: 0.8159 - val_loss: 1.3254 - val_accuracy: 0.6236 - lr: 9.6294e-05\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 9.602499478632206e-05.\n",
            "Epoch 29/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.8254\n",
            "Epoch 29: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 137ms/step - loss: 0.5780 - accuracy: 0.8254 - val_loss: 1.3457 - val_accuracy: 0.6292 - lr: 9.6025e-05\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 9.574732905574518e-05.\n",
            "Epoch 30/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.8336\n",
            "Epoch 30: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5663 - accuracy: 0.8336 - val_loss: 1.3433 - val_accuracy: 0.6236 - lr: 9.5747e-05\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 9.54609483704231e-05.\n",
            "Epoch 31/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.8345\n",
            "Epoch 31: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5609 - accuracy: 0.8345 - val_loss: 1.3282 - val_accuracy: 0.6236 - lr: 9.5461e-05\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 9.516593512495273e-05.\n",
            "Epoch 32/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5376 - accuracy: 0.8383\n",
            "Epoch 32: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5376 - accuracy: 0.8383 - val_loss: 1.3391 - val_accuracy: 0.6292 - lr: 9.5166e-05\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 9.486237893382716e-05.\n",
            "Epoch 33/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.8378\n",
            "Epoch 33: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5252 - accuracy: 0.8378 - val_loss: 1.3677 - val_accuracy: 0.6348 - lr: 9.4862e-05\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 9.455036212378557e-05.\n",
            "Epoch 34/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.8423\n",
            "Epoch 34: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5289 - accuracy: 0.8423 - val_loss: 1.3654 - val_accuracy: 0.6292 - lr: 9.4550e-05\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 9.422998149135443e-05.\n",
            "Epoch 35/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.8498\n",
            "Epoch 35: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.5064 - accuracy: 0.8498 - val_loss: 1.3729 - val_accuracy: 0.6292 - lr: 9.4230e-05\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 9.390132654389606e-05.\n",
            "Epoch 36/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8528\n",
            "Epoch 36: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4902 - accuracy: 0.8528 - val_loss: 1.3647 - val_accuracy: 0.6292 - lr: 9.3901e-05\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 9.35644940029564e-05.\n",
            "Epoch 37/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.8606\n",
            "Epoch 37: val_accuracy improved from 0.63483 to 0.65169, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4755 - accuracy: 0.8606 - val_loss: 1.3639 - val_accuracy: 0.6517 - lr: 9.3564e-05\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 9.321958055153534e-05.\n",
            "Epoch 38/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.8592\n",
            "Epoch 38: val_accuracy did not improve from 0.65169\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4703 - accuracy: 0.8592 - val_loss: 1.3678 - val_accuracy: 0.6404 - lr: 9.3220e-05\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 9.286669008251971e-05.\n",
            "Epoch 39/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.8674\n",
            "Epoch 39: val_accuracy did not improve from 0.65169\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4538 - accuracy: 0.8674 - val_loss: 1.3660 - val_accuracy: 0.6461 - lr: 9.2867e-05\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 9.25059191997089e-05.\n",
            "Epoch 40/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4569 - accuracy: 0.8592\n",
            "Epoch 40: val_accuracy improved from 0.65169 to 0.65730, saving model to vgg16_1.h5\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4569 - accuracy: 0.8592 - val_loss: 1.3735 - val_accuracy: 0.6573 - lr: 9.2506e-05\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 9.213737171538456e-05.\n",
            "Epoch 41/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.8683\n",
            "Epoch 41: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4374 - accuracy: 0.8683 - val_loss: 1.3749 - val_accuracy: 0.6517 - lr: 9.2137e-05\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 9.176115140047293e-05.\n",
            "Epoch 42/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8686\n",
            "Epoch 42: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4287 - accuracy: 0.8686 - val_loss: 1.3781 - val_accuracy: 0.6517 - lr: 9.1761e-05\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 9.13773692300919e-05.\n",
            "Epoch 43/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4292 - accuracy: 0.8669\n",
            "Epoch 43: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4292 - accuracy: 0.8669 - val_loss: 1.3895 - val_accuracy: 0.6461 - lr: 9.1377e-05\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 9.098612889035443e-05.\n",
            "Epoch 44/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4179 - accuracy: 0.8730\n",
            "Epoch 44: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.4179 - accuracy: 0.8730 - val_loss: 1.4112 - val_accuracy: 0.6404 - lr: 9.0986e-05\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 9.058754127016436e-05.\n",
            "Epoch 45/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.3983 - accuracy: 0.8777\n",
            "Epoch 45: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 32s 138ms/step - loss: 0.3983 - accuracy: 0.8777 - val_loss: 1.4184 - val_accuracy: 0.6292 - lr: 9.0588e-05\n",
            "Epoch 45: early stopping\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit(np.array(x_train1), np.array(y_train), validation_data=(x_val,y_val), epochs=50, \n",
        "                 callbacks = [checkpoint,early, keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)], batch_size = 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ddt8VivqFwkD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "1bc75933-eacf-4d63-9bb8-a45455f40134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180/180 [==============================] - 31s 159ms/step - loss: 0.2303 - accuracy: 0.9505\n",
            "6/6 [==============================] - 3s 509ms/step - loss: 0.5937 - accuracy: 0.8090\n",
            "Train: 0.951, Test: 0.809\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x_test = tf.stack(x_test)\n",
        "x_test1 = tf.keras.applications.vgg16.preprocess_input(x_test)\n",
        "y_test = tf.stack(y_test)\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(x_train1, y_train, verbose=1)\n",
        "_, test_acc = model.evaluate(x_test1, y_test, verbose=1)\n",
        "\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "train_error = []\n",
        "test_error = []\n",
        "\n",
        "for accuracy in hist.history[\"accuracy\"]:\n",
        "    train_error.append(1-accuracy)\n",
        "for accuracy in hist.history[\"val_accuracy\"]:\n",
        "    test_error.append(1-accuracy)\n",
        "\n",
        "# plot training history\n",
        "pyplot.plot(train_error , label='train')\n",
        "pyplot.plot(test_error, label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(x_test1) \n",
        "y_pred=np.argmax(y_pred, axis=1)\n",
        "y_test2=np.argmax(y_test, axis=1)\n",
        "\n",
        "print(confusion_matrix(y_test2, y_pred))\n",
        "print(classification_report(y_test2, y_pred))\n"
      ],
      "metadata": {
        "id": "xHrNGVUjJESZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ce1849-d1df-4e80-884e-0df7b8f03b41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 155ms/step\n",
            "[[5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 7 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 7 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 7 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 8 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 7 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 7 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 0 0 0 0 0 5 0 0 0 0 1 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 5 1 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 5 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 7 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.71      0.83         7\n",
            "           1       0.70      0.78      0.74         9\n",
            "           2       1.00      0.78      0.88         9\n",
            "           3       0.78      0.78      0.78         9\n",
            "           4       1.00      0.78      0.88         9\n",
            "           5       0.67      0.89      0.76         9\n",
            "           6       0.88      0.78      0.82         9\n",
            "           7       0.78      0.78      0.78         9\n",
            "           8       1.00      0.89      0.94         9\n",
            "           9       0.82      1.00      0.90         9\n",
            "          10       0.78      0.78      0.78         9\n",
            "          11       0.90      1.00      0.95         9\n",
            "          12       0.62      0.56      0.59         9\n",
            "          13       1.00      1.00      1.00         9\n",
            "          14       0.80      0.89      0.84         9\n",
            "          15       0.83      0.56      0.67         9\n",
            "          16       0.82      1.00      0.90         9\n",
            "          17       0.62      0.56      0.59         9\n",
            "          18       0.64      0.78      0.70         9\n",
            "          19       0.80      0.89      0.84         9\n",
            "\n",
            "    accuracy                           0.81       178\n",
            "   macro avg       0.82      0.81      0.81       178\n",
            "weighted avg       0.82      0.81      0.81       178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ASzt6JJ21hd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108e7ff2-4baf-4c0e-d7ef-c78a326e7ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0.h5\n",
            "21834768/21834768 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb0 (Functional)  (None, 1000)             5330571   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1000)              0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1000)             4000      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                20020     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,355,011\n",
            "Trainable params: 22,440\n",
            "Non-trainable params: 5,332,571\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model = EfficientNetB0(weights='imagenet', drop_connect_rate=0.2)\n",
        "model = keras.Sequential()\n",
        "model.add(base_model)\n",
        "for layer in model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "# model.add(keras.layers.Flatten())\n",
        "# model.add(keras.layers.Dense(20, activation=('relu')))\n",
        "model.add(keras.layers.Dense(20, activation=('softmax')))\n",
        "\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "model.build(input_shape=(224,224,3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train2 = tf.keras.applications.efficientnet.preprocess_input(x_train)\n",
        "initial_learning_rate = 0.01\n",
        "epochs = 100\n",
        "decay = initial_learning_rate / epochs\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return lr * 1 / (1 + decay * epoch)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best.h5\", monitor='val_accuracy', verbose=1, \n",
        "                             save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n"
      ],
      "metadata": {
        "id": "OIcGmRJWFgLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65650fa6-2c5d-408b-9d28-b756cdfda2d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(np.array(x_train2), np.array(y_train), validation_data=(x_val,y_val), epochs=50, \n",
        "                 callbacks = [checkpoint,early, keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)], batch_size = 25)"
      ],
      "metadata": {
        "id": "swZjmp-AGrO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3721b53-f27f-4d04-ef57-55c78f7d3ed9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
            "Epoch 1/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.9657 - accuracy: 0.0544\n",
            "Epoch 1: val_accuracy improved from -inf to 0.06180, saving model to best.h5\n",
            "230/230 [==============================] - 20s 62ms/step - loss: 2.9657 - accuracy: 0.0544 - val_loss: 2.9693 - val_accuracy: 0.0618 - lr: 1.0000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 9.998999847394012e-05.\n",
            "Epoch 2/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.8998 - accuracy: 0.0995\n",
            "Epoch 2: val_accuracy improved from 0.06180 to 0.14045, saving model to best.h5\n",
            "230/230 [==============================] - 13s 54ms/step - loss: 2.8998 - accuracy: 0.0995 - val_loss: 2.8772 - val_accuracy: 0.1404 - lr: 9.9990e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 9.997000630676428e-05.\n",
            "Epoch 3/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.8094 - accuracy: 0.1690\n",
            "Epoch 3: val_accuracy improved from 0.14045 to 0.25843, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.8094 - accuracy: 0.1690 - val_loss: 2.6969 - val_accuracy: 0.2584 - lr: 9.9970e-05\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 9.994002396931107e-05.\n",
            "Epoch 4/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.6929 - accuracy: 0.2514\n",
            "Epoch 4: val_accuracy improved from 0.25843 to 0.37640, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.6929 - accuracy: 0.2514 - val_loss: 2.5158 - val_accuracy: 0.3764 - lr: 9.9940e-05\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 9.990006173048161e-05.\n",
            "Epoch 5/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 2.5704 - accuracy: 0.2976\n",
            "Epoch 5: val_accuracy improved from 0.37640 to 0.47191, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.5705 - accuracy: 0.2976 - val_loss: 2.3419 - val_accuracy: 0.4719 - lr: 9.9900e-05\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 9.985013712739302e-05.\n",
            "Epoch 6/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.4460 - accuracy: 0.3455\n",
            "Epoch 6: val_accuracy improved from 0.47191 to 0.51685, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.4460 - accuracy: 0.3455 - val_loss: 2.1866 - val_accuracy: 0.5169 - lr: 9.9850e-05\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 9.979026041855706e-05.\n",
            "Epoch 7/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 2.3258 - accuracy: 0.3825\n",
            "Epoch 7: val_accuracy improved from 0.51685 to 0.56742, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.3250 - accuracy: 0.3831 - val_loss: 2.0337 - val_accuracy: 0.5674 - lr: 9.9790e-05\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 9.972045640012098e-05.\n",
            "Epoch 8/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 2.2262 - accuracy: 0.3963\n",
            "Epoch 8: val_accuracy improved from 0.56742 to 0.57865, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.2258 - accuracy: 0.3965 - val_loss: 1.9088 - val_accuracy: 0.5787 - lr: 9.9720e-05\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 9.964074258818005e-05.\n",
            "Epoch 9/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 2.1359 - accuracy: 0.4110\n",
            "Epoch 9: val_accuracy did not improve from 0.57865\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 2.1359 - accuracy: 0.4110 - val_loss: 1.8035 - val_accuracy: 0.5787 - lr: 9.9641e-05\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 9.955114376124448e-05.\n",
            "Epoch 10/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 2.0505 - accuracy: 0.4363\n",
            "Epoch 10: val_accuracy improved from 0.57865 to 0.58989, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 2.0499 - accuracy: 0.4362 - val_loss: 1.7193 - val_accuracy: 0.5899 - lr: 9.9551e-05\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 9.945169195661194e-05.\n",
            "Epoch 11/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.9790 - accuracy: 0.4344\n",
            "Epoch 11: val_accuracy improved from 0.58989 to 0.61236, saving model to best.h5\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.9802 - accuracy: 0.4336 - val_loss: 1.6506 - val_accuracy: 0.6124 - lr: 9.9452e-05\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 9.934241193081636e-05.\n",
            "Epoch 12/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.9332 - accuracy: 0.4410\n",
            "Epoch 12: val_accuracy improved from 0.61236 to 0.62921, saving model to best.h5\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.9333 - accuracy: 0.4411 - val_loss: 1.5921 - val_accuracy: 0.6292 - lr: 9.9342e-05\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 9.922334296497487e-05.\n",
            "Epoch 13/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.8716 - accuracy: 0.4617\n",
            "Epoch 13: val_accuracy did not improve from 0.62921\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.8721 - accuracy: 0.4613 - val_loss: 1.5507 - val_accuracy: 0.6292 - lr: 9.9223e-05\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 9.909451705800132e-05.\n",
            "Epoch 14/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.8437 - accuracy: 0.4564\n",
            "Epoch 14: val_accuracy did not improve from 0.62921\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.8434 - accuracy: 0.4566 - val_loss: 1.5032 - val_accuracy: 0.6236 - lr: 9.9095e-05\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 9.895598072759898e-05.\n",
            "Epoch 15/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.7952 - accuracy: 0.4711\n",
            "Epoch 15: val_accuracy did not improve from 0.62921\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.7952 - accuracy: 0.4711 - val_loss: 1.4748 - val_accuracy: 0.6236 - lr: 9.8956e-05\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 9.880776594277179e-05.\n",
            "Epoch 16/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.7510 - accuracy: 0.4683\n",
            "Epoch 16: val_accuracy did not improve from 0.62921\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.7519 - accuracy: 0.4681 - val_loss: 1.4401 - val_accuracy: 0.6292 - lr: 9.8808e-05\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 9.864992645276061e-05.\n",
            "Epoch 17/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.7401 - accuracy: 0.4631\n",
            "Epoch 17: val_accuracy did not improve from 0.62921\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.7403 - accuracy: 0.4631 - val_loss: 1.4175 - val_accuracy: 0.6236 - lr: 9.8650e-05\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 9.84825087217336e-05.\n",
            "Epoch 18/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.7131 - accuracy: 0.4741\n",
            "Epoch 18: val_accuracy improved from 0.62921 to 0.63483, saving model to best.h5\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.7123 - accuracy: 0.4746 - val_loss: 1.3945 - val_accuracy: 0.6348 - lr: 9.8483e-05\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 9.830555919530664e-05.\n",
            "Epoch 19/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.6882 - accuracy: 0.4877\n",
            "Epoch 19: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.6872 - accuracy: 0.4882 - val_loss: 1.3759 - val_accuracy: 0.6348 - lr: 9.8306e-05\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 9.81191315627121e-05.\n",
            "Epoch 20/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.6613 - accuracy: 0.4950\n",
            "Epoch 20: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.6615 - accuracy: 0.4949 - val_loss: 1.3573 - val_accuracy: 0.6348 - lr: 9.8119e-05\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 9.792328675318427e-05.\n",
            "Epoch 21/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.6442 - accuracy: 0.5039\n",
            "Epoch 21: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.6448 - accuracy: 0.5033 - val_loss: 1.3428 - val_accuracy: 0.6292 - lr: 9.7923e-05\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 9.771807841092672e-05.\n",
            "Epoch 22/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.6239 - accuracy: 0.5001\n",
            "Epoch 22: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.6240 - accuracy: 0.5000 - val_loss: 1.3309 - val_accuracy: 0.6348 - lr: 9.7718e-05\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 9.750356741871807e-05.\n",
            "Epoch 23/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.6110 - accuracy: 0.5015\n",
            "Epoch 23: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.6126 - accuracy: 0.5010 - val_loss: 1.3178 - val_accuracy: 0.6348 - lr: 9.7504e-05\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 9.727982189430099e-05.\n",
            "Epoch 24/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.5872 - accuracy: 0.5122\n",
            "Epoch 24: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.5872 - accuracy: 0.5122 - val_loss: 1.3110 - val_accuracy: 0.6292 - lr: 9.7280e-05\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 9.704690992823636e-05.\n",
            "Epoch 25/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.5791 - accuracy: 0.5171\n",
            "Epoch 25: val_accuracy did not improve from 0.63483\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.5791 - accuracy: 0.5171 - val_loss: 1.3019 - val_accuracy: 0.6348 - lr: 9.7047e-05\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 9.680489958391672e-05.\n",
            "Epoch 26/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.5537 - accuracy: 0.5245\n",
            "Epoch 26: val_accuracy improved from 0.63483 to 0.64045, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 1.5534 - accuracy: 0.5244 - val_loss: 1.2893 - val_accuracy: 0.6404 - lr: 9.6805e-05\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 9.655385889757983e-05.\n",
            "Epoch 27/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.5480 - accuracy: 0.5219\n",
            "Epoch 27: val_accuracy did not improve from 0.64045\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.5484 - accuracy: 0.5218 - val_loss: 1.2893 - val_accuracy: 0.6404 - lr: 9.6554e-05\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 9.629386313468771e-05.\n",
            "Epoch 28/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.5367 - accuracy: 0.5235\n",
            "Epoch 28: val_accuracy improved from 0.64045 to 0.64607, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 1.5355 - accuracy: 0.5239 - val_loss: 1.2771 - val_accuracy: 0.6461 - lr: 9.6294e-05\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 9.602499478632206e-05.\n",
            "Epoch 29/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.5106 - accuracy: 0.5326\n",
            "Epoch 29: val_accuracy did not improve from 0.64607\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.5103 - accuracy: 0.5328 - val_loss: 1.2699 - val_accuracy: 0.6348 - lr: 9.6025e-05\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 9.574732905574518e-05.\n",
            "Epoch 30/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.5059 - accuracy: 0.5289\n",
            "Epoch 30: val_accuracy did not improve from 0.64607\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.5056 - accuracy: 0.5293 - val_loss: 1.2614 - val_accuracy: 0.6461 - lr: 9.5747e-05\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 9.54609483704231e-05.\n",
            "Epoch 31/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.5040 - accuracy: 0.5324\n",
            "Epoch 31: val_accuracy improved from 0.64607 to 0.65169, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 1.5040 - accuracy: 0.5324 - val_loss: 1.2595 - val_accuracy: 0.6517 - lr: 9.5461e-05\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 9.516593512495273e-05.\n",
            "Epoch 32/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4924 - accuracy: 0.5401\n",
            "Epoch 32: val_accuracy improved from 0.65169 to 0.65730, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 1.4924 - accuracy: 0.5397 - val_loss: 1.2533 - val_accuracy: 0.6573 - lr: 9.5166e-05\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 9.486237893382716e-05.\n",
            "Epoch 33/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4841 - accuracy: 0.5389\n",
            "Epoch 33: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4840 - accuracy: 0.5389 - val_loss: 1.2499 - val_accuracy: 0.6573 - lr: 9.4862e-05\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 9.455036212378557e-05.\n",
            "Epoch 34/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4883 - accuracy: 0.5450\n",
            "Epoch 34: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4877 - accuracy: 0.5453 - val_loss: 1.2498 - val_accuracy: 0.6517 - lr: 9.4550e-05\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 9.422998149135443e-05.\n",
            "Epoch 35/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.4534 - accuracy: 0.5432\n",
            "Epoch 35: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4534 - accuracy: 0.5432 - val_loss: 1.2430 - val_accuracy: 0.6573 - lr: 9.4230e-05\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 9.390132654389606e-05.\n",
            "Epoch 36/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4397 - accuracy: 0.5527\n",
            "Epoch 36: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.4399 - accuracy: 0.5526 - val_loss: 1.2345 - val_accuracy: 0.6517 - lr: 9.3901e-05\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 9.35644940029564e-05.\n",
            "Epoch 37/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.4447 - accuracy: 0.5418\n",
            "Epoch 37: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.4447 - accuracy: 0.5418 - val_loss: 1.2326 - val_accuracy: 0.6461 - lr: 9.3564e-05\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 9.321958055153534e-05.\n",
            "Epoch 38/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4289 - accuracy: 0.5497\n",
            "Epoch 38: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4304 - accuracy: 0.5490 - val_loss: 1.2301 - val_accuracy: 0.6517 - lr: 9.3220e-05\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 9.286669008251971e-05.\n",
            "Epoch 39/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.4273 - accuracy: 0.5533\n",
            "Epoch 39: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4273 - accuracy: 0.5533 - val_loss: 1.2308 - val_accuracy: 0.6461 - lr: 9.2867e-05\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 9.25059191997089e-05.\n",
            "Epoch 40/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4337 - accuracy: 0.5513\n",
            "Epoch 40: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.4326 - accuracy: 0.5517 - val_loss: 1.2281 - val_accuracy: 0.6517 - lr: 9.2506e-05\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 9.213737171538456e-05.\n",
            "Epoch 41/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4291 - accuracy: 0.5499\n",
            "Epoch 41: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.4290 - accuracy: 0.5500 - val_loss: 1.2229 - val_accuracy: 0.6573 - lr: 9.2137e-05\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 9.176115140047293e-05.\n",
            "Epoch 42/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.4209 - accuracy: 0.5527\n",
            "Epoch 42: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.4199 - accuracy: 0.5530 - val_loss: 1.2195 - val_accuracy: 0.6517 - lr: 9.1761e-05\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 9.13773692300919e-05.\n",
            "Epoch 43/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.3942 - accuracy: 0.5617\n",
            "Epoch 43: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.3942 - accuracy: 0.5617 - val_loss: 1.2144 - val_accuracy: 0.6573 - lr: 9.1377e-05\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 9.098612889035443e-05.\n",
            "Epoch 44/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.3965 - accuracy: 0.5613\n",
            "Epoch 44: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.3965 - accuracy: 0.5613 - val_loss: 1.2157 - val_accuracy: 0.6573 - lr: 9.0986e-05\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 9.058754127016436e-05.\n",
            "Epoch 45/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.3805 - accuracy: 0.5712\n",
            "Epoch 45: val_accuracy did not improve from 0.65730\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.3789 - accuracy: 0.5720 - val_loss: 1.2081 - val_accuracy: 0.6573 - lr: 9.0588e-05\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 9.018172445763116e-05.\n",
            "Epoch 46/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.3674 - accuracy: 0.5612\n",
            "Epoch 46: val_accuracy improved from 0.65730 to 0.66292, saving model to best.h5\n",
            "230/230 [==============================] - 12s 53ms/step - loss: 1.3671 - accuracy: 0.5610 - val_loss: 1.2042 - val_accuracy: 0.6629 - lr: 9.0182e-05\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 8.976878925120389e-05.\n",
            "Epoch 47/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.3913 - accuracy: 0.5616\n",
            "Epoch 47: val_accuracy did not improve from 0.66292\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.3913 - accuracy: 0.5611 - val_loss: 1.2030 - val_accuracy: 0.6629 - lr: 8.9769e-05\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 8.934884640521953e-05.\n",
            "Epoch 48/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.3681 - accuracy: 0.5737\n",
            "Epoch 48: val_accuracy improved from 0.66292 to 0.66854, saving model to best.h5\n",
            "230/230 [==============================] - 12s 54ms/step - loss: 1.3681 - accuracy: 0.5737 - val_loss: 1.2003 - val_accuracy: 0.6685 - lr: 8.9349e-05\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 8.892202111232468e-05.\n",
            "Epoch 49/50\n",
            "230/230 [==============================] - ETA: 0s - loss: 1.3575 - accuracy: 0.5707\n",
            "Epoch 49: val_accuracy did not improve from 0.66854\n",
            "230/230 [==============================] - 12s 51ms/step - loss: 1.3575 - accuracy: 0.5707 - val_loss: 1.1988 - val_accuracy: 0.6573 - lr: 8.8922e-05\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 8.848843127485384e-05.\n",
            "Epoch 50/50\n",
            "229/230 [============================>.] - ETA: 0s - loss: 1.3483 - accuracy: 0.5680\n",
            "Epoch 50: val_accuracy did not improve from 0.66854\n",
            "230/230 [==============================] - 12s 52ms/step - loss: 1.3477 - accuracy: 0.5685 - val_loss: 1.1969 - val_accuracy: 0.6573 - lr: 8.8488e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test2 = tf.keras.applications.efficientnet.preprocess_input(x_test)\n",
        "y_test2 = tf.stack(y_test)\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(np.array(x_train), np.array(y_train), verbose=1)\n",
        "_, test_acc = model.evaluate(np.array(x_test), np.array(y_test2), verbose=1)\n",
        "\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "train_error = []\n",
        "test_error = []\n",
        "\n",
        "for accuracy in hist.history[\"accuracy\"]:\n",
        "    train_error.append(1-accuracy)\n",
        "for accuracy in hist.history[\"val_accuracy\"]:\n",
        "    test_error.append(1-accuracy)\n",
        "\n",
        "# plot training history\n",
        "pyplot.plot(train_error, label='train')\n",
        "pyplot.plot(test_error, label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "99AfaoBpG4aS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "7ea012db-7e42-4293-aa2a-40d280059830"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180/180 [==============================] - 11s 58ms/step - loss: 1.0589 - accuracy: 0.6854\n",
            "6/6 [==============================] - 1s 145ms/step - loss: 1.1969 - accuracy: 0.6573\n",
            "Train: 0.685, Test: 0.657\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(np.array(x_test)) \n",
        "y_pred=np.argmax(y_pred, axis=1)\n",
        "y_test=np.argmax(y_test, axis=1)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "3syR6SCvIdsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ac312f-5aaa-422c-e302-957cc59c45f8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 2s 64ms/step\n",
            "[[4 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
            " [0 8 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
            " [1 0 0 5 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 2 2 0 0 0 0 1 0 0 0 0 1 0 0 3 0]\n",
            " [0 0 1 0 0 3 1 0 1 0 0 0 0 0 1 2 0 0 0 0]\n",
            " [0 1 0 0 0 0 7 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 7 0 1 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 1 1 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 1 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 6 0 1 0 0 0 2 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 2 4 3 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 7 0 0 1 0 0]\n",
            " [0 1 0 0 1 2 0 1 2 1 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 8 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 2 0 1 0 0 5 0 0]\n",
            " [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 5 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.57      0.67         7\n",
            "           1       0.67      0.89      0.76         9\n",
            "           2       0.58      0.78      0.67         9\n",
            "           3       1.00      0.56      0.71         9\n",
            "           4       0.33      0.22      0.27         9\n",
            "           5       0.38      0.33      0.35         9\n",
            "           6       0.88      0.78      0.82         9\n",
            "           7       0.78      0.78      0.78         9\n",
            "           8       0.67      0.89      0.76         9\n",
            "           9       0.78      0.78      0.78         9\n",
            "          10       0.78      0.78      0.78         9\n",
            "          11       0.75      1.00      0.86         9\n",
            "          12       0.55      0.67      0.60         9\n",
            "          13       1.00      0.44      0.62         9\n",
            "          14       0.54      0.78      0.64         9\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.89      0.89      0.89         9\n",
            "          17       0.62      0.56      0.59         9\n",
            "          18       0.45      0.56      0.50         9\n",
            "          19       0.67      0.89      0.76         9\n",
            "\n",
            "    accuracy                           0.66       178\n",
            "   macro avg       0.66      0.66      0.64       178\n",
            "weighted avg       0.65      0.66      0.64       178\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}